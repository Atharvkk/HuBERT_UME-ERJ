% !TEX program = lualatex
\documentclass[conference]{IEEEtran}

% --- LUALATEX UNICODE & FONT SUPPORT ---
\usepackage{fontspec}

% Set the main font to one that supports the IPA characters in your table.
\setmainfont{FreeSerif}

% --- PACKAGES ---
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs} 
\usepackage{multirow} 
\usepackage{longtable}
\usepackage{unicode-math} 
\usepackage{IEEEtrantools}

\begin{document}

% --- TITLE AND AUTHORS ---
\title{Measuring Phoneme-Level Pronunciation Deviations in Japanese Learners of English Using Self-Supervised Speech Representations}

\author{\IEEEauthorblockN{Atharv Kulkarni : Lead Researcher | System Architect\\ }
\IEEEauthorblockA{\textit{India International School in Japan} \\
Tokyo, Japan \\
\{s2013236\}@iisjapan.com}
}

\maketitle

% --- ABSTRACT ---
\begin{abstract}
Research suggests a part of the reason for the limited English Proficiency, or bilingualism, in Japan, is tied to a societal 'fear' of mispronouncing certain words in English. Accurate pronunciation feedback is therefore essential for Japanese learners of English, but traditional Computer-Assisted Language Learning (CALL) software typically provides general feedback, lacking the necessary regional nuance required for greater effectiveness. This paper provides insight into the phoneme-level deviations of Japanese speakers of English, compared to a baseline of American English speakers, conducted on the UME-ERJ corpus by the NII-SRC (Japan's National Institute of Information - Speech research consortium) using a self-supervised speech representation (SSSR) model, which, to the best of our knowledge, represents the first attempt to quantify pronunciation deviations in Japanese-English using this state-of-the-art method.
\end{abstract}

\begin{IEEEkeywords}
Self-supervised learning, HuBERT, Japanese-English, Phoneme Analysis.
\end{IEEEkeywords}

% --- SECTION I: INTRODUCTION ---
\section{Introduction}
Despite its global economic standing, Japan has consistently been ranked as 'low' or 'very low' in the context of english proficiency, holding the 96th position on the EF EPI index in 2025\cite{b13}. Prior research suggests that this is not due to exposure, as English is a standard subject in the Japanese education system, but, in fact, due to a cultural 'shyness' or a 'lack of willingness to speak English', predominantly because of the fear of mispronunciation\cite{b2,b3}. This psychological barrier negatively impacts overall proficiency, where learners hesitate to speak English due to the fear of phonetic mistakes, or appearing 'bad' at the language. Computer-Assisted Language Learning systems attempt to bridge this, and can be further improved by reliable region specific data.

Most modern CALL systems rely on supervised speech-to-text (STT) or Mel Frequency Cepstral Coefficients (MFCC) to judge pronunciation against an internal metric\cite{b4}. One notable downside of this method is not being able to classify deep phoneme-level nuances, notably the distinction between liquid phonemes (/l/ and /r/) and fricatives (/$\theta$/ and /ð/)\cite{b3,b9,b10}, which are known to be difficult for Japanese Native (L1) speakers. Supervised models need massive amounts of labeled non-native speech data, which is scarce and expensive to produce on such scales\cite{b12,b15}. Thus, there is a need for a self-supervised metric that can quantify 'phonetic distance' without manual transcription.  

In this paper, we address the previously mentioned challenges by using Self-Supervised Learning (SSL) representations\cite{b6,b7}. By using layer 9 of HuBERT, we can extract high precision latent feature encodings which depict the underlying structure of speech. The dataset utilized is the UME-ERJ corpus, a robust set containing a total of 69,888 (35,277 Female | 34,611 Male) usable non-native speaker data files, spread across 102 Female and 100 male volunteers. A collection of 17,055 (10,126 Female | 6,929 Male) native samples were used to establish a baseline. Transcripts were obtained from the provided documentation, and were sliced into individual text files to ensure compatibility with Montreal-Forced-Aligner (MFA) for phenome-level classification, which were then used to sorting individual phenomes post HuBERT encoding.

% --- SECTION II: METHOD ---
\section{Method}

\subsection{Classifying the data}

The UME-ERJ dataset\cite{b0} is trifurcated into three smaller units, hence refered to as UME-ERJ 1, UME-ERJ 2, and UME-ERJ 3. UME-ERJ 1, UME-ERJ 2 consisted of speakers from 19 Japanese Universities speaking from a set of English words which were both general sentences and words/sentences predicted to be 'hard' for L1 Japanese speakers. From each university, each speaker was identified with an ID number in the form M/F-XY, where XY are unique to the person. the contents of the audio said by the speaker were from a common set of sentences labeled S(0-8)-(001-125) and words W(0-5)-(001-228). 

UME-ERJ 3 consisted of 20 American Speakers of English, 12 Female speakers and 8 Male speakers. The content was different to the sentences/words uttered by the Japanese volunteers.

We also created a unified, cleaned version of the provided transcripts for sentences uttered by both American English (AE) and Japanese English (JE) speakers. Using a Python script, we matched the \texttt{speaker\_id} in the transcript files to the corresponding audio filename and inserted the transcript content into a new text file. This preprocessing step ensured compatibility between the \texttt{.wav} and \texttt{.txt} files required by the Montreal Forced Aligner (MFA). Finally, all files were systematically renamed to standardize processing, using the format \texttt{S(A)\_(PQR)\_(SQT)\_F|M(XY).extension}, where A is sentence group number, PQR is sentence track, STQ is university ID, and XY is speaker ID. The extensions were either \texttt{.wav} or \texttt{.txt}.


% --- FIGURE 1 PLACEHOLDER ---
%\begin{figure}[t]
%\centering
% \includegraphics[width=\linewidth]{path/to/image.png} 
% Using a placeholder image below:
%\includegraphics[width=\linewidth, height=5cm]{example-image-a} 

%\caption{The HuBERT approach predicts hidden cluster assignments of the masked frames generated by one or more iterations of k-means clustering. (Placeholder graphic)}
%\label{fig:hubert_arch}
%\end{figure}

\subsection{Identifying Phonemes present and their Boundaries}

Montreal Forced Alignment (MFA) was used to identify Phonemes present in the \texttt{.wav} files and their region boundaries\cite{b4}. Taking the \texttt{.wav} file and a \texttt{.txt} file as inputs, it ran MFCC localization to output \texttt{.textgrid} files with multiple layers of phonemic information\cite{b12}. 


\subsection{Using the Self-Supervised Speech Representation Model}

Layer 9 of HuBERT\cite{b7} was utilized due to being ranked as having the best classification for our use case\cite{b7,b8,b9}; Phoneme-level analysis. Using the raw encodings from HuBERT, a python script utilized the previously created \texttt{.textgrid} files to locate specific phonemes, and using the mapped boundary times, sliced the HuBERT encodings from start to step in 20ms intervals. The mean of the collected zone was then utilized to be a part the final global $\mu_{\text{0}}$, the token-weighed phoneme centroid, calculated by again averaging the parts, to arrive at a final phoneme value in a 768 dimension latent vector value\cite{b7}. This process was repeated 6 times, for JE male/female, AE male/female, JE male+female, AE male+female

The data is stored in a pickle \texttt{.pkl} binary file.
\\
For further analysis, the following terms were also calculated to ensure data quality: $\mu_{\text{speaker-norm}}$: the speaker-balanced centroid, $\sigma_{\text{global}}$ and $\sigma_{\text{inter-speaker}}$.

\subsection{Data Analysis}
Data Analysis was also conducted using the python data analysis and graphing libraries of Seaborn, Matplotlib, SciPy and Numpy. The process involved depickling of the \texttt{.pkl} files and comparing the vector datapoints using cosine distance\cite{b14} between JE and the AE baseline across all metrics.

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{pipeline.png}
\caption{System Architecture of the Proposed Evaluation Pipeline. The process integrates a bifurcated workflow: (1) temporal phoneme boundary identification via the Montreal Forced Aligner (MFA) using compiled transcripts and raw audio, and (2) high-dimensional feature extraction through the HuBERT self-supervised model. The final stage performs a temporal-to-latent mapping to generate speaker-specific phoneme collections in a 768-dimensional vector space.}
\label{fig:pipeline}
\end{figure}

\section{Experimental Details}

\subsection{Experimental Setup}

The primary computation device used was a personal computer, having an Intel i-9 13900HX processor, RTX 4070 mobile GPU for cuda acceleration, and 32GB DDR5 5600 RAM for \texttt{numpy} headroom. 

Python was the primary programming language used, with the standard Machine Learning libraries being used.

MFA was installed in a separate anaconda virtual environment to avoid conflicts with the main algorithm.
MFA computation took 730.143 seconds for 35,277 JE Female tracks, and 643.312 seconds for JE Male tracks.
\\
HuBERT encoding took roughly 1 minute per 1,000 files with my setup (16.67 items/second\cite{b7})

\section{Results}

The obtained data were analyzed across four major categories:

\begin{enumerate}
    \item \textbf{Japanese Female vs. American Female:} To determine the average phonemic deviations between female speakers across both L1 and L2 cohorts.
    \item \textbf{Japanese Male vs. American Male:} To determine the average phonemic deviations between male speakers across both L1 and L2 cohorts.
    \item \textbf{Sex-Based Deviation Comparison:} A comparative analysis of [Japanese Male $|$ American Male] deviations against [Japanese Female $|$ American Female] deviations to observe how biological sex may correlate with phonetic convergence
    \item \textbf{Aggregate Composite Analysis:} A comparison of the total baseline composite (Male + Female) against the total Japanese English composite (Male + Female) to quantify the global phonetic distance.
\end{enumerate}

\subsection{\textbf{Japanese Female vs. American Female} }
In the first comparison, it a few notable phonemes are instantly discernible due to high phonetic drifts compared to the baseline. The compiled data can be seen below:


\begin{table}[ht!]
\centering
\caption{Phonetic Drift Comparison: Japanese Female vs. American Female Baseline}
\label{tab:phoneme_drift}
\begin{tabular}{lc|lc|lc}
\toprule
\textbf{Phoneme} & \textbf{Drift} & \textbf{Phoneme} & \textbf{Drift} & \textbf{Phoneme} & \textbf{Drift} \\
\midrule
ɾ   & 6.8086 & ow  & 2.0469 & ʃ   & 1.6758 \\
m   & 5.1133 & tʰ  & 2.0332 & ŋ   & 1.6738 \\
ɡʷ  & 4.8633 & ɒː  & 2.0078 & ɛ   & 1.6689 \\
ɫ   & 3.2676 & kʷ  & 2.0020 & f   & 1.6387 \\
ɟʷ  & 3.1191 & v   & 1.9932 & iː  & 1.6387 \\
ɝ   & 3.0586 & ɐ   & 1.9316 & ɔj  & 1.6279 \\
ð   & 2.9727 & æ   & 1.9277 & pʰ  & 1.6182 \\
ɾʲ  & 2.8867 & ɪ   & 1.9229 & d   & 1.5928 \\
l   & 2.7500 & dʲ  & 1.8936 & z   & 1.5732 \\
ʎ   & 2.7422 & aw  & 1.8701 & i   & 1.5703 \\
ɚ   & 2.5625 & w   & 1.8633 & h   & 1.5352 \\
t   & 2.5410 & ʉː  & 1.8262 & kʰ  & 1.5166 \\
vʲ  & 2.5117 & dʒ  & 1.8262 & ej  & 1.5166 \\
tʷ  & 2.4883 & ə   & 1.8252 & pʲ & 1.4893 \\
ʒ   & 2.4844 & tʃ  & 1.8154 & cʰ  & 1.4561 \\
n   & 2.4512 & ʊ   & 1.8135 & bʲ  & 1.3496 \\
θ   & 2.4277 & cʷ  & 1.8008 & ɲ   & 1.3320 \\
tʲ  & 2.3457 & c   & 1.7920 & p   & 1.3125 \\
ɹ   & 2.0703 & fʲ  & 1.7910 & k   & 1.2764 \\
ɑː  & 2.0488 & ʉ   & 1.7822 & j   & 1.2656 \\
\midrule
\multicolumn{6}{l}{\textbf{Summary Statistics}} \\
\midrule
\multicolumn{3}{l}{Mean Global Drift} & \multicolumn{3}{r}{2.0625} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{Female_Comparison_plot.png}
\caption{Female Cohort Comparison}
\label{fig:pipeline}
\end{figure}

\subsection{\textbf{Japanese Male vs. American Male} }
We will perform the analysis similar to the one in section 1, with the results being classified below:

From the preliminary analysis of the graph and the table, it can be seen that while the males report an otherwise lower deviation from the American Baseline, the deviation in the ɾ phoneme is significantly greater than that from the female cohort. The ɡʷ deviation is also seen to be greater than that of the female cohort. The deviation from m is also not as significant. 

\begin{table}[ht!]
\centering
\caption{Phonetic Drift Comparison: Japanese Male vs. American Male Baseline}
\label{tab:phoneme_drift_male}
\begin{tabular}{lc|lc|lc}
\toprule
\textbf{Phoneme} & \textbf{Drift} & \textbf{Phoneme} & \textbf{Drift} & \textbf{Phoneme} & \textbf{Drift} \\
\midrule
ɾ   & 8.9453 & v   & 2.1973 & ʉ   & 1.7783 \\
ɡʷ  & 5.3320 & kʷ  & 2.1797 & i   & 1.7617 \\
ɟʷ  & 3.9238 & tʰ  & 2.1563 & ɒ   & 1.7305 \\
m   & 3.8613 & ɪ   & 2.0957 & ɛ   & 1.7070 \\
l   & 3.6367 & ɐ   & 2.0508 & ʃ   & 1.6514 \\
ɫ   & 3.5059 & æ   & 2.0352 & pʲ  & 1.6396 \\
ʎ   & 3.4961 & fʲ  & 2.0195 & kʰ  & 1.6191 \\
ɝ   & 3.4238 & dʲ  & 2.0098 & ɔj  & 1.6182 \\
ð   & 3.3066 & ə   & 2.0098 & h   & 1.6113 \\
ɾʲ  & 3.2617 & z   & 2.0039 & pʰ  & 1.6035 \\
ɚ   & 3.0176 & c   & 2.0000 & iː  & 1.5947 \\
vʲ  & 2.8086 & ow  & 1.9502 & s   & 1.5020 \\
tʷ  & 2.8027 & ɒː  & 1.9316 & ej  & 1.5020 \\
t   & 2.6309 & w   & 1.9229 & ɲ   & 1.4893 \\
θ   & 2.6133 & tʃ  & 1.8877 & cʰ  & 1.4297 \\
n   & 2.5879 & f   & 1.8545 & k   & 1.3955 \\
tʲ  & 2.5859 & ʊ   & 1.8359 & bʲ  & 1.3926 \\
ʒ   & 2.4004 & ʉː  & 1.8281 & p   & 1.3584 \\
ɑː  & 2.3086 & ŋ   & 1.8203 & ɡ   & 1.3516 \\
ɹ   & 2.2754 & dʒ  & 1.8037 & j   & 1.3467 \\
cʷ  & 2.2734 & ɑ   & 1.7969 & aj  & 1.3057 \\
d   & 1.7920 & aw  & 1.7900 & ɟ   & 1.2959 \\
\midrule
\multicolumn{6}{l}{\textbf{Summary Statistics}} \\
\midrule
\multicolumn{3}{l}{Mean Global Drift} & \multicolumn{3}{r}{2.2363} \\

\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{Male_Comparison_plot.png}
\caption{Male Cohort Comparison}
\label{fig:pipeline}
\end{figure}

\vspace{\fill}

\subsection{\textbf{[Japanese Male | American Male] vs [Japanese Female | American Female]} }

This general comparison can help identify which sex has more issues with pronunciation in general. As with the earlier comparsion, it can be observed that whiles males in general have less deviation in most phonemes, the significantly greater variation in the few phonenes which show a higher rate of deviation skews the average drift of the males higher than that of the females.

\begin{table}[ht!]
\centering
\caption{Sex-Based Phonetic Deviation Comparison}
\label{tab:sex_comparison}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Japanese Male Average Drift   & 2.2363  \\
Japanese Female Average Drift & 2.0625  \\
\midrule
\textbf{Difference (Male - Female)} & \textbf{0.1738}  \\
\bottomrule
\end{tabular}
\end{table}

\vspace{\fill}

\subsection{\textbf{[Japanese Composite (Male + Female) ] vs [ American Composite (Male + Female)]} }

An aggregate analysis can help in identifying the common deviations for all speakers of L1 Japanese, providing the general overview of pronunciation deviations, and the most likely mispronunciations\cite{b8,b9}.

\begin{table}[ht!]
\centering
\caption{Phonetic Drift Comparison: Global Aggregate Composite}
\label{tab:global_aggregate}
\begin{tabular}{lc|lc|lc}
\toprule
\textbf{Phoneme} & \textbf{Drift} & \textbf{Phoneme} & \textbf{Drift} & \textbf{Phoneme} & \textbf{Drift} \\
\midrule
ɾ & 6.1016 & ow & 1.9785 & ɛ & 1.6406 \\
ɡʷ & 4.9375 & cʷ & 1.9746 & ʃ & 1.6172 \\
m & 4.2461 & ɪ & 1.9717 & i & 1.6113 \\
ɫ & 3.3535 & ɐ & 1.9541 & pʰ & 1.5830 \\
ɟʷ & 3.3320 & æ & 1.9531 & ɔj & 1.5684 \\
ɝ & 3.1914 & ɒː & 1.9297 & iː & 1.5674 \\
l & 3.1445 & dʲ & 1.9150 & h & 1.5371 \\
ð & 3.1309 & ə & 1.8887 & kʰ & 1.5293 \\
ʎ & 3.0820 & fʲ & 1.8633 & pʲ & 1.5254 \\
ɾʲ & 2.9531 & w & 1.8623 & ej & 1.4717 \\
ɚ & 2.7422 & c & 1.8584 & cʰ & 1.4082 \\
vʲ & 2.6309 & aw & 1.8154 & ɲ & 1.3594 \\
tʷ & 2.5801 & ʊ & 1.8096 & bʲ & 1.3359 \\
t & 2.5234 & tʃ & 1.7822 & p & 1.2881 \\
θ & 2.5020 & ʉː & 1.7773 & k & 1.2822 \\
tʲ & 2.4160 & dʒ & 1.7725 & j & 1.2725 \\
n & 2.3711 & ʉ & 1.7393 & ɡ & 1.2559 \\
ʒ & 2.3633 & f & 1.7207 & ɟ & 1.2227 \\
ɹ & 2.1406 & ɑ & 1.7051 & s & 1.2090 \\
ɑː & 2.0957 & z & 1.7021 & b & 1.2021 \\
tʰ & 2.0625 & ɒ & 1.6865 & aj & 1.1875 \\
v & 2.0625 & ŋ & 1.6621 & mʲ & 1.1641 \\
kʷ & 2.0352 & d & 1.6562 &  &  \\
\midrule
\multicolumn{6}{l}{\textbf{Summary Statistics}} \\
\midrule
\multicolumn{3}{l}{Mean Global Drift} & \multicolumn{3}{r}{2.0723} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{Global_Aggregate_plot.png}
\caption{Aggregate Comparison}
\label{fig:pipeline}
\end{figure}


\vspace{\fill}
\subsection{\textbf{Analysis of inferred data} }
Analyzing the aggregate; we can classify the phonemes into three subcategories;

\begin{enumerate} \item \textbf{High Deviation: Drift > 3.0} 

\begin{itemize} \item Consonants showing extreme divergence: ɾ (6.1016), ɡʷ (4.9375), m (4.2461), ɫ (3.3535), ɟʷ (3.3320), l (3.1445), ð (3.1309), and ʎ (3.0820). \item Rhotic vowels: ɝ (3.1914)\cite{b3,b10,b11}. \end{itemize}

\item \textbf{Medium Deviation: 3.0 > Drift > 1.7}
\begin{itemize}
    \item Flaps and R-colored sounds: ɾʲ (2.9531) and ɚ (2.7422).{b3,b10}
    \item Labialized and Dental consonants: vʲ (2.6309), tʷ (2.5801), t (2.5234), θ (2.5020), tʲ (2.4160), and n (2.3711).
    \item Sibilants and Fricatives: ʒ (2.3633), ɹ (2.1406), tʰ (2.0625), v (2.0625), kʷ (2.0352), cʷ (1.9746), fʲ (1.8633), c (1.8584), tʃ (1.7822), dʒ (1.7725), f (1.7207), and z (1.7021){b3,b10}.
    \item Vowels and Diphthongs: ɑː (2.0957), ow (1.9785), ɪ (1.9717), ɐ (1.9541), æ (1.9531), ɒː (1.9297), dʲ (1.9150), ə (1.8887), w (1.8623), aw (1.8154), ʊ (1.8096), ʉː (1.7773), ʉ (1.7393), and ɑ (1.7051){b3,b9}.
\end{itemize}

\item \textbf{Low Deviation: Drift < 1.7}
\begin{itemize}
    \item Vowels: ɒ (1.6865), ɛ (1.6406), i (1.6113), and iː (1.5674){b11,b12}
    \item Consonants and Glides: ŋ (1.6621), d (1.6563), ʃ (1.6172), pʰ (1.5830), ɔj (1.5684), h (1.5371), kʰ (1.5293), pʲ (1.5254), ej (1.4717), cʰ (1.4082), ɲ (1.3594), bʲ (1.3359), p (1.2881), k (1.2822), j (1.2725), ɡ (1.2559), ɟ (1.2227), s (1.2090), b (1.2021), aj (1.1875), and mʲ (1.1641).{b9,b10}
\end{itemize}
\end{enumerate}

This is largely consistent with other phonetic studies conducted on L1 Japanese speakers with L2 English.

\subsection{\textbf{Known Limitations} }

The greatest known shortcoming of this study is that the utterances were not identical across both the American speakers and Japanese speakers. It has the possibility to hurt HuBERT embeddings and may decrease accuracy, even when it is strictly a one-to-one phoneme comparison.

The reliance on MFA for phonemic classification may also disrupt the temporal phonemic classification of Japanese speakers, having been trained almost entirely on American English Data. 

Lastly, there is the lack of tests with Mahalanobis diagonal mapping. The hardware, as mentioned in Methodology, was deemed insufficiently powerful for the 768x768 images, and was not worth producing in the author's resource-strained state. 

\vspace{\fill}

\section{Conclusion}

This work presented a large-scale, phoneme-level analysis of pronunciation deviations in Japanese learners of English using self-supervised speech representations. By leveraging HuBERT latent embeddings in conjunction with forced phoneme alignment, we quantified phonetic drift between Japanese English (JE) speakers and an American English (AE) baseline without reliance on supervised pronunciation scoring or handcrafted acoustic features. The results demonstrate that SSL-based representations are sufficiently sensitive to capture fine-grained phonemic deviations that are well-documented in second language acquisition literature, including liquid consonants, rhotics, labialized consonants, and dental fricatives.{7,b8,b15}

Across all experimental conditions, the alveolar flap /ɾ/ consistently exhibited the highest deviation, confirming its central role in Japanese–English phonological interference{b5,b13}. Additional high-drift phonemes, such as /ɡʷ/, /ɫ/, /ð/, and rhotic vowels, further reinforce known articulatory and perceptual mismatches between the two phonological systems. Sex-based analyses revealed that while male speakers exhibited lower average deviation across most phonemes, a small subset of highly divergent phonemes disproportionately increased their global drift metric, highlighting the importance of phoneme-wise rather than aggregate evaluation.

The aggregate composite analysis demonstrated that pronunciation deviation in Japanese learners is not uniformly distributed across the phoneme inventory, but instead concentrated within a relatively small subset of sounds. This finding has direct implications for Computer-Assisted Language Learning systems, suggesting that targeted, phoneme-specific feedback may be substantially more effective than holistic pronunciation scoring{b15,b16}. Importantly, the use of self-supervised representations eliminates the need for large-scale labeled non-native speech corpora, significantly lowering the barrier for extensible and language-agnostic pronunciation assessment{b6,b7}.

Future work may extend this framework to temporal modeling of phoneme transitions, integration with prosodic features, or real-time feedback systems. Additionally, adapting the methodology to other L1–L2 pairings would further validate the generalizability of SSL-based phonetic distance metrics. Overall, this study demonstrates that self-supervised speech models provide a robust and scalable foundation for objective, data-driven pronunciation analysis in second language learning contexts.

\vspace{\fill}

\section{Acknowledgements}
The original idea of studying Japanese Learners of English came from working on an app to aid in pronunciation with a focus on Japan; a CALL of our own, for the Intel Digital Readiness Project, mentor-led program held weekly at our school. I would like to acknowledge our school, India International School in Japan for providing an environment where curiosity was encouraged, and thrived. My teachers also played a large part in motivating me to conduct independant research. I would also like to present my gratidute to my fellow teammates, Yatharth Jain, Zayaan Nanji and Shaunak Jadhav, for their help in the initial stages of the project. Zayaan Nanji in particular has been of great help in taking our findings and applying them within tha app.

Lastly, this project was possible due to the co-operation provided by the NII-SRC, in granting us access to the UME-ERJ dataset, which was an integral part of the research.

\begin{thebibliography}{00}

\bibitem{b0}
N. Minematsu, Y. Tomiyama, K. Yoshimoto, K. Shimizu, S. Nakagawa, 
M. Dantsuji, and S. Makino,
``English Speech Database Read by Japanese Students (UME-ERJ),'' 
developed under the Grant-in-Aid for Scientific Research (MEXT, Japan),
2001--2003.
Primary descriptions appear in:
\textit{Proc. ASJ Autumn Meeting}, pp.199--200, 2001;
\textit{Proc. COCOSDA Workshop}, pp.76--81, 2001;
\textit{Proc. LREC}, pp.896--903, 2002;
\textit{J. Acoust. Soc. Jpn.}, vol.59, no.6, pp.345--350, 2003;
and
\textit{J. Jpn. Soc. Educ. Technol.}, vol.27, no.3, pp.259--272, 2003.


\bibitem{b1} K. Ohata, “Potential sources of anxiety for Japanese learners of English: Preliminary case interviews with five Japanese college students in the U.S.,” TESL-EJ, vol. 9, no. 3, 2005.
\bibitem{b2} P. Cutrone, “Overcoming Japanese EFL learners’ fear of speaking,” Language Studies Working Papers, vol. 1, pp. 55–63, 2009.

\bibitem{b3} G. E. Oh, S. Guion-Anderson, K. Aoyama, J. S. Flege, R. Akahane-Yamada, and T. Yamada, “The effect of age of acquisition on first- and second-language vowel production,” J. Phonetics, vol. 39, no. 4, pp. 585–600, 2011.

\bibitem{b4} M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger, “Montreal Forced Aligner: Trainable text-speech alignment using Kaldi,” in Proc. Interspeech 2017, pp. 498–502, 2017.

\bibitem{b5} P. Sofroniev and Ç. Çöltekin, “Phonetic vector representations for sound sequence alignment,” in Proc. 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, Brussels, Belgium, 2018, pp. 111–116.

\bibitem{b6} A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” in Advances in Neural Information Processing Systems, vol. 33, 2020, pp. 12449–12460.

\bibitem{b7} W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451–3460, 2021.

\bibitem{b8} D. Wells, H. Tang, and K. Richmond, “Phonetic analysis of self-supervised representations of English speech,” in Proc. Interspeech 2022, pp. 3583–3587, 2022.

\bibitem{b9} K. Martin, J. Gauthier, C. Breiss, and R. Levy, “Probing self-supervised speech models for phonetic and phonemic information: a case study in aspiration,” in Proc. Interspeech 2023, 2023, pp. 2355–2359.

\bibitem{b10} A. M. Diaz, “Phonetics applied: An acoustic analysis of Japanese L2 English,” in P. Ferguson et al. (eds.), Learning from Students, Educating Teachers – Research and Practice (Proc. JALT 2022), JALT, 2023, pp. 67–75.

\bibitem{b11} T. Nagamine, “Dynamic tongue movements in L1 Japanese and L2 English liquids,” in Proc. 20th Int. Congress of Phonetic Sciences (ICPhS), 2023.

\bibitem{b12} S. Williams, P. Foulkes, and V. Hughes, “Analysis of forced aligner performance on L2 English speech,” Speech Communication, vol. 158, pp. 1–14, 2024.

\bibitem{b13} D. A. Patankar, “The evolution of English language learning in Japan: A historical and statistical analysis,” Int. J. Prog. Res. Eng. Mgmt. Sci., vol. 5, no. 1, pp. 1411–1420, 2025.

\bibitem{b14} Q. Wang and K. A. Lee, “Cosine scoring with uncertainty for neural speaker embedding,” arXiv:2403.06404 [cs.SD], 2024.

\bibitem{b15} M. Shahin, J. Epps, and B. Ahmed, “Phonological level Wav2Vec 2.0-based mispronunciation detection and diagnosis method,” Speech Communication, vol. 173, pp. 1–15, 2025.

\bibitem{b16} A. Fort and B. Tyers, “Wav2Vec2-BERT for ESL pronunciation error detection in isiZulu,” in Proc. Interspeech 2025, 2025, pp. 1234–1238.

\end{thebibliography}


\end{document}